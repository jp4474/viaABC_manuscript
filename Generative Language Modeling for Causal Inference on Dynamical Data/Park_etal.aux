\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{tavare1997inferring}
\citation{marjoram2003markov}
\citation{toni2009approximate}
\citation{robert2008adaptivity}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{aakesson2021convolutional}
\citation{fefferman2016testing}
\citation{tavare1997inferring}
\citation{toni2009approximate}
\citation{balestriero2023cookbook}
\@writefile{toc}{\contentsline {paragraph}{ABC}{3}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ABC Rejection}{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sequential ABC}{3}{section*.4}\protected@file@percent }
\citation{devlin2019bert}
\citation{he2022masked}
\citation{yao2022masked}
\citation{li2023ti}
\citation{cheng2023timemae}
\citation{jordan1999introduction}
\citation{Blei_2017}
\citation{Blei_2017}
\citation{kingma2013auto}
\@writefile{toc}{\contentsline {paragraph}{Masked Modeling}{4}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Varitional Autoencoder}{4}{section*.6}\protected@file@percent }
\citation{}
\citation{mckay2000comparison}
\@writefile{toc}{\contentsline {paragraph}{Latent variables as summary statistics in Sequential ABC}{5}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training:}{5}{section*.9}\protected@file@percent }
\citation{higgins2017beta}
\citation{rabanser2020effectiveness}
\citation{ansari2024chronos}
\citation{salinas2020deepar}
\citation{rabanser2020effectiveness}
\citation{ansari2024chronos}
\citation{ansari2024chronos}
\citation{dosovitskiy2020image,he2022masked}
\@writefile{toc}{\contentsline {paragraph}{TSM VAE:}{7}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Temporal Patching}{7}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variational Latent Space Representation}{7}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Tokenized encoding and sequential Monte Carlo to generate posterior distribution of parameters}. {\color  {magenta}{this figure will be the full schematic of VIA-ABC}}.   During pretraining, a random subset of temporal patches (e.g., 15\%) is masked out. The encoder is applied to the visible patches. The CLS token (red) and Mask tokens (gray) are introduced after the encoder, and the full set of encoded patches along with the mask tokens undergoes the reparametrization trick. This is then processed by a small decoder that reconstructs the original patches. After pretraining, the decoder is discarded, and the encoder is applied to uncorrupted multivariate time series (full sets of patches) for viaABC.\relax }}{8}{figure.caption.13}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:enter-label}{{1}{8}{\textbf {Tokenized encoding and sequential Monte Carlo to generate posterior distribution of parameters}. \blue {this figure will be the full schematic of VIA-ABC}. \\ During pretraining, a random subset of temporal patches (e.g., 15\%) is masked out. The encoder is applied to the visible patches. The CLS token (red) and Mask tokens (gray) are introduced after the encoder, and the full set of encoded patches along with the mask tokens undergoes the reparametrization trick. This is then processed by a small decoder that reconstructs the original patches. After pretraining, the decoder is discarded, and the encoder is applied to uncorrupted multivariate time series (full sets of patches) for viaABC.\relax }{figure.caption.13}{}}
\citation{marin2012approximate}
\citation{simola2021adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}VIA-ABC}{9}{subsection.1.1}\protected@file@percent }
\citation{ishida2015cosmoabc}
\citation{simola2021adaptive}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces viaABC\relax }}{10}{algorithm.1}\protected@file@percent }
\citation{lotka1925elements}
\citation{volterra1928variations}
\citation{toni2009approximate}
\citation{dinh2024approximate}
\citation{dinh2024approximate}
\citation{devlin2019bert}
\citation{dd}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Means and variances of marginal posterior distributions for the deterministic Lotka-Volterra model from viaABC, ABC-DRF, ABC-SMC and ABC-SMC-DRF. The best result for each statistic across different algorithms is in bold (E(a), E(b) closest to true values (a, b) = (1, 1), and lowest variance in each statistic). Results are taken from Dinh et al. \citep  {dinh2024approximate}.\relax }}{13}{table.caption.16}\protected@file@percent }
\newlabel{tab:lotka_volterra_results}{{1}{13}{Means and variances of marginal posterior distributions for the deterministic Lotka-Volterra model from viaABC, ABC-DRF, ABC-SMC and ABC-SMC-DRF. The best result for each statistic across different algorithms is in bold (E(a), E(b) closest to true values (a, b) = (1, 1), and lowest variance in each statistic). Results are taken from Dinh et al. \citep {dinh2024approximate}.\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experiments}{13}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Main Properties}{13}{subsection.2.1}\protected@file@percent }
\citation{gillespie1977exact}
\bibstyle{unsrt}
\bibdata{refs}
\bibcite{tavare1997inferring}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{14}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{14}{section.4}\protected@file@percent }
\bibcite{marjoram2003markov}{{2}{}{{}}{{}}}
\bibcite{toni2009approximate}{{3}{}{{}}{{}}}
\bibcite{robert2008adaptivity}{{4}{}{{}}{{}}}
\bibcite{aakesson2021convolutional}{{5}{}{{}}{{}}}
\bibcite{fefferman2016testing}{{6}{}{{}}{{}}}
\bibcite{balestriero2023cookbook}{{7}{}{{}}{{}}}
\bibcite{devlin2019bert}{{8}{}{{}}{{}}}
\bibcite{he2022masked}{{9}{}{{}}{{}}}
\bibcite{yao2022masked}{{10}{}{{}}{{}}}
\bibcite{li2023ti}{{11}{}{{}}{{}}}
\bibcite{cheng2023timemae}{{12}{}{{}}{{}}}
\bibcite{jordan1999introduction}{{13}{}{{}}{{}}}
\bibcite{Blei_2017}{{14}{}{{}}{{}}}
\bibcite{kingma2013auto}{{15}{}{{}}{{}}}
\bibcite{dosovitskiy2020image}{{16}{}{{}}{{}}}
\bibcite{mckay2000comparison}{{17}{}{{}}{{}}}
\bibcite{higgins2017beta}{{18}{}{{}}{{}}}
\bibcite{rabanser2020effectiveness}{{19}{}{{}}{{}}}
\bibcite{ansari2024chronos}{{20}{}{{}}{{}}}
\bibcite{salinas2020deepar}{{21}{}{{}}{{}}}
\bibcite{marin2012approximate}{{22}{}{{}}{{}}}
\bibcite{simola2021adaptive}{{23}{}{{}}{{}}}
\bibcite{ishida2015cosmoabc}{{24}{}{{}}{{}}}
\bibcite{lotka1925elements}{{25}{}{{}}{{}}}
\bibcite{volterra1928variations}{{26}{}{{}}{{}}}
\bibcite{gillespie1977exact}{{27}{}{{}}{{}}}
\bibcite{dinh2024approximate}{{28}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{17}
