\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{tavare1997inferring}
\citation{marjoram2003markov}
\citation{toni2009approximate}
\citation{robert2008adaptivity}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{aakesson2021convolutional}
\citation{fefferman2016testing}
\citation{tavare1997inferring}
\citation{toni2009approximate}
\citation{balestriero2023cookbook}
\citation{devlin2019bert}
\citation{he2022masked}
\citation{yao2022masked}
\@writefile{toc}{\contentsline {paragraph}{ABC}{3}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ABC Rejection}{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sequential ABC}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Masked Modeling}{3}{section*.5}\protected@file@percent }
\citation{li2023ti}
\citation{cheng2023timemae}
\citation{jordan1999introduction}
\citation{Blei_2017}
\citation{Blei_2017}
\citation{kingma2013auto}
\@writefile{toc}{\contentsline {paragraph}{Varitional Autoencoder}{4}{section*.6}\protected@file@percent }
\citation{}
\citation{mckay2000comparison}
\@writefile{toc}{\contentsline {paragraph}{Training:}{5}{section*.8}\protected@file@percent }
\citation{higgins2017beta}
\citation{rabanser2020effectiveness}
\citation{ansari2024chronos}
\citation{salinas2020deepar}
\citation{rabanser2020effectiveness}
\citation{ansari2024chronos}
\citation{ansari2024chronos}
\citation{dosovitskiy2020image,he2022masked}
\@writefile{toc}{\contentsline {paragraph}{TSM VAE:}{6}{section*.9}\protected@file@percent }
\citation{marin2012approximate}
\citation{simola2021adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Tokenized encoding and sequential Monte Carlo to generate posterior distribution of parameters}. {\color  {magenta}{this figure will be the full schematic of VIA-ABC}}.   During pretraining, a random subset of temporal patches (e.g., 15\%) is masked out. The encoder is applied to the visible patches. The CLS token (red) and Mask tokens (gray) are introduced after the encoder, and the full set of encoded patches along with the mask tokens undergoes the reparametrization trick. This is then processed by a small decoder that reconstructs the original patches. After pretraining, the decoder is discarded, and the encoder is applied to uncorrupted multivariate time series (full sets of patches) for viaABC.\relax }}{7}{figure.caption.12}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:enter-label}{{1}{7}{\textbf {Tokenized encoding and sequential Monte Carlo to generate posterior distribution of parameters}. \blue {this figure will be the full schematic of VIA-ABC}. \\ During pretraining, a random subset of temporal patches (e.g., 15\%) is masked out. The encoder is applied to the visible patches. The CLS token (red) and Mask tokens (gray) are introduced after the encoder, and the full set of encoded patches along with the mask tokens undergoes the reparametrization trick. This is then processed by a small decoder that reconstructs the original patches. After pretraining, the decoder is discarded, and the encoder is applied to uncorrupted multivariate time series (full sets of patches) for viaABC.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Temporal Patching}{7}{section*.10}\protected@file@percent }
\@writefile{to